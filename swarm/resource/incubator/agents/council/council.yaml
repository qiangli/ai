#!/usr/bin/env ai /agent:council/council --script

#
# LLM Council (3-stage orchestration) as a self-contained YAML agent pack.
# Refactor goal: move orchestration out of Go tool code into sub-agents.
# Source concept: /Users/liqiang/workspace/poc/llm-council/README.md (top section "LLM Council")
#
# Stages:
# 1) First opinions: council members answer independently
# 2) Review: council members rank anonymized answers
# 3) Chairman: synthesizes final response from stage1 + stage2
#
# This pack avoids implementing logic in Go. The only "tool" used is ai:call_llm.
#

---
pack: "council"
log_level: "info"

agents:
  # ------------------------------------------------------------
  # Orchestrator
  # ------------------------------------------------------------
  - name: "council"
    display: "üèõÔ∏è LLM Council"
    description: "3-stage multi-model council: parallel answers ‚Üí anonymized peer ranking ‚Üí chairman synthesis (agent-based)."
    model: "default/any"
    instruction: |
      You orchestrate an LLM Council process exactly as described below.

      Council definition (from README):
      - Stage 1: multiple models answer independently; show each response.
      - Stage 2: each model reviews and ranks anonymized responses.
      - Stage 3: a Chairman model synthesizes a single final answer.

      Inputs:
      - The user's message is the query.

      Configuration (via environment variables):
      - COUNCIL_MODELS: JSON array of model aliases/IDs (e.g. ["default/L2","gemini/L2"]) ; optional.
      - CHAIRMAN_MODEL: model alias/ID for synthesis; optional.
      - TITLE_MODEL: model alias/ID for title generation; optional.
      - COUNCIL_SIZE: integer; if set, truncate council list.

      Defaults if env vars are missing:
      - council models: ["default/L2","gemini/L2","anthropic/L2","xai/L2"]
      - chairman: "default/L3"
      - title model: "default/L1"

      You MUST follow these stages:

      Stage 1 (First opinions)
      - For each council model, call sub-agent council/member_answer with that model and the user query.
      - Collect successful results into stage1 as: [{model, response}].
      - If all members fail, return JSON with empty stage1/stage2 and stage3.response = error.

      Stage 2 (Review & ranking)
      - Create anonymized labels Response A, Response B, ... for each stage1 response.
      - Build label_to_model mapping: {"Response A": "<model>", ...}.
      - Build a single ranking prompt (verbatim format requirements below) and send it to each council model via council/member_rank.
      - Each stage2 item: {model, ranking, parsed_ranking}.
      - Parse parsed_ranking from ranking text using council/ranking_parser (extract ordered list of "Response X"; prefer FINAL RANKING section).
      - Compute aggregate_rankings as average position per original model using council/aggregate_ranker.

      Stage 3 (Chairman synthesis)
      - Send chairman model a synthesis prompt containing original question, stage1 (with model names), stage2 (with model names).
      - Get stage3 as {model, response}.

      Title
      - Use council/title_generator to create a 3‚Äì5 word title (best effort).

      Output
      - Return a JSON object with keys: stage1, stage2, stage3, metadata.
      - metadata must include: label_to_model, aggregate_rankings, title, council_models, chairman_model, title_model.

      Ranking prompt format requirements (must be enforced in stage2 prompt):
      - The model MUST include a section starting with exactly: "FINAL RANKING:"
      - Then a numbered list where each line is: "1. Response A" etc.
      - No extra text in the ranking section.

      Tool/agent usage rules:
      - Use agent-based orchestration.
      - You may call ai:call_llm directly only inside helper agents (member_answer/member_rank/chairman/title_generator) OR when strictly necessary.
      - Keep the returned JSON strictly machine-readable.

    functions:
      - "agent:council/member_answer"
      - "agent:council/member_rank"
      - "agent:council/chairman"
      - "agent:council/title_generator"
      - "agent:council/ranking_parser"
      - "agent:council/aggregate_ranker"

    entrypoint:
      - "ai:new_agent"
      - "ai:build_query"
      - "ai:build_prompt"
      - "ai:call_llm"


  # ------------------------------------------------------------
  # Stage 1: council member answers
  # ------------------------------------------------------------
  - name: "member_answer"
    display: "Council Member (Answer)"
    description: "Ask a specific model to answer the user query."
    model: "default/any"
    instruction: |
      You are a council member answering a user question.

      You MUST call ai:call_llm exactly once with:
      - model: {{ .model }}
      - query: {{ .query }}

      Return ONLY a JSON object: {"model":"...","response":"..."}.
      - model must echo the provided model.
      - response must be the LLM answer text.
      - If the LLM call fails, return {"model":"...","error":"..."}.
    functions:
      - "ai:call_llm"


  # ------------------------------------------------------------
  # Stage 2: council member ranks anonymized responses
  # ------------------------------------------------------------
  - name: "member_rank"
    display: "Council Member (Rank)"
    description: "Ask a specific model to critique and rank anonymized responses."
    model: "default/any"
    instruction: |
      You are a council member reviewing anonymized responses.

      You MUST call ai:call_llm exactly once with:
      - model: {{ .model }}
      - query: a ranking prompt that includes:
        Question: {{ .query }}
        Anonymized responses block: {{ .responses_text }}
        And the required ranking format instructions.

      Ranking instructions MUST be included verbatim:
      IMPORTANT: Your final ranking MUST be formatted EXACTLY as follows:
      - Start with the line "FINAL RANKING:" (all caps, with colon)
      - Then list the responses from best to worst as a numbered list
      - Each line should be: number, period, space, then ONLY the response label (e.g., "1. Response A")
      - Do not add any other text or explanations in the ranking section

      Return ONLY JSON: {"model":"...","ranking":"..."}.
      If the LLM call fails, return {"model":"...","error":"..."}.
    functions:
      - "ai:call_llm"


  # ------------------------------------------------------------
  # Stage 3: chairman synthesis
  # ------------------------------------------------------------
  - name: "chairman"
    display: "Chairman"
    description: "Synthesize final answer from stage1+stage2."
    model: "default/any"
    instruction: |
      You are the Chairman of an LLM Council.

      You MUST call ai:call_llm exactly once with:
      - model: {{ .model }}
      - query: a chairman prompt containing:
        Original Question: {{ .query }}
        STAGE 1 - Individual Responses: {{ .stage1_text }}
        STAGE 2 - Peer Rankings: {{ .stage2_text }}

      Return ONLY JSON: {"model":"...","response":"..."}.
      If the LLM call fails, return {"model":"...","error":"..."}.
    functions:
      - "ai:call_llm"


  # ------------------------------------------------------------
  # Title generation (best effort)
  # ------------------------------------------------------------
  - name: "title_generator"
    display: "Title Generator"
    description: "Generate a short conversation title (3‚Äì5 words)."
    model: "default/any"
    instruction: |
      Generate a very short title (3-5 words maximum) that summarizes the following question.
      The title should be concise and descriptive. Do not use quotes or punctuation in the title.

      Question: {{ .query }}

      You MUST call ai:call_llm exactly once with:
      - model: {{ .model }}
      - query: the instructions above.

      Return ONLY JSON: {"title":"..."}.
      If the LLM call fails, return {"title":"New Conversation"}.
    functions:
      - "ai:call_llm"


  # ------------------------------------------------------------
  # Ranking parser (ported from council.py logic, but LLM-based)
  # ------------------------------------------------------------
  - name: "ranking_parser"
    display: "Ranking Parser"
    description: "Parse a ranking text to an ordered list of Response labels (deterministic)."
    model: "default/any"
    instruction: |
      Parse the provided ranking text into an ordered list of labels like ["Response A","Response B"].

      Deterministic algorithm (no external calls):
      1) If the text contains the substring "FINAL RANKING:", take the substring after its first occurrence as the ranking section.
      2) In that ranking section, extract all matches of the regex: \d+\.\s*Response [A-Z]
         - From each match, extract just: Response <LETTER>
         - If any were found, that is the parsed ranking.
      3) Otherwise, extract all matches of the regex: Response [A-Z] from the ranking section.
      4) If "FINAL RANKING:" is not present, fall back to extracting Response [A-Z] from the entire text.
      5) Deduplicate while preserving order.

      Input:
      - ranking_text: {{ .ranking_text }}

      Output: ONLY JSON {"parsed_ranking":[...]}.

      You MAY call ai:call_llm zero times; do not call tools.
    functions: []


  - name: "aggregate_ranker"
    display: "Aggregate Ranker"
    description: "Compute aggregate rankings from parsed rankings and label_to_model mapping."
    model: "default/any"
    instruction: |
      Compute aggregate rankings.

      Inputs:
      - stage2: JSON array of objects with keys {model, parsed_ranking} where parsed_ranking is ["Response A", ...]
      - label_to_model: JSON object mapping response label to original model

      Method:
      - For each stage2 item, for each label at position i (1-based), map label->model via label_to_model and record the position.
      - For each original model, compute average position across all recorded positions.
      - Round to 2 decimals.
      - Produce array sorted ascending by average_rank:
        [{"model":"...","average_rank":1.75,"rankings_count":4}, ...]

      Return ONLY JSON: {"aggregate_rankings":[...]}.
    functions:
      - "ai:call_llm"
