###
# https://github.com/RUC-NLPIR/DeepAgent/tree/main
pack: "eval"
log_level: "verbose"
max_time: 900
agents:
  - name: "tool"
    display: "‚öôÔ∏è Eval Tool"
    description:
    model: "llm/any"
    instruction: |
      You are an evaluation agent. you are given 4 tools for evaluation based on user's input/assistant's answers and other information.
      1) agent:eval/check_answer - determine if the answer correctly addresses the query
      2) agent:eval/parse_answer - analyze the answer structure to determine if the task was solved
      3) agent:eval/check_solvable - determine if the given task is solvable with the available tools
      4) agent:eval/select_answer - compare multiple answers to the same query and select the better one.
    functions:
      - "agent:eval/check_answer"
      - "agent:eval/parse_answer"
      - "agent:eval/check_solvable"
      - "agent:eval/select_answer"

  # main entrypoint
  - name: "eval"
    display: "üí¨ Q&A"
    description: "Provides concise and reliable answers across diverse topics, ranging from local system intricacies to comprehensive web-based information, ensuring clarity and accuracy."
    model: llm/any
    embeb:
      - agent:eval/tool
    instruction: |
      You are a helpful assistant. Use available tools only when needed,
      responding with just the required tool call format.
      For regular questions, answer clearly and concisely,
      focusing on providing relevant and informative responses.
    functions:
      - "web:*"
      - "ddg:*"

  - name: "check_answer"
    display: "Check Answer"
    description: |
      This evaluation assistant is tasked with assessing if an answer accurately addresses a given query.
      The assistant categorizes responses as "Solved," "Unsolved," or "Unsure," based on accuracy and completeness.
    model: "llm/any"
    message: |
      Please provide your assessment:
    instruction: |
      #! mime-type=text/x-go-template
      You are an evaluation assistant.
      Please determine if the answer correctly addresses the query.

      Query: {{.query}}

      Answer: {{.answer}}

      Please analyze if the answer is correct, incorrect, or unclear. Consider:
      1. Does the answer directly address the query?
      2. Is the answer accurate and complete?
      3. Does the answer provide the requested information or solution?

      Respond with one of the following:
      - "Solved": The answer correctly and completely addresses the query
      - "Unsolved": The answer is incorrect or does not address the query
      - "Unsure": The answer is unclear or partially addresses the query

  - name: "parse_answer"
    display: "Parse Answer"
    description: |
      This evaluation assistant's role is to assess task completion by analyzing answer structure, tool usage, reasoning,
      and final response, categorizing results as "Solved," "Unsolved," or "Unsure" based on these criteria.
    model: "llm/any"
    message: |
      Please provide your assessment:
    instruction: |
      #! mime-type=text/x-go-template
      You are an evaluation assistant.
      Please analyze the detailed answer structure to determine if the task was solved.

      Query: {{.query}}

      Answer Details: {{.result}}

      Please examine the answer structure, tool usage, and final response to determine if the task was successfully completed.

      Consider:
      1. Were appropriate tools used correctly?
      2. Did the agent follow a logical reasoning process?
      3. Was a final answer provided that addresses the query?
      4. Were there any errors or incomplete steps?

      Respond with one of the following:
      - "Solved": The task was successfully completed with appropriate tool usage and a correct final answer
      - "Unsolved": The task was not completed due to errors, incorrect tool usage, or missing final answer
      - "Unsure": The completion status is unclear due to partial information or ambiguous results

  - name: "check_solvable"
    display: "Check Solvable"
    description: |
      This is an evaluation assistant to determine the solvability of tasks.
      It analyzes task clarity, tool availability, scope, and limitations,
      responding with "Solvable," "Unsolvable," or "Unsure" based on the assessment.
    model: "llm/any"
    message: |
      Please provide your assessment:
    instruction: |
      #! mime-type=text/x-go-template
      You are an evaluation assistant.
      Please determine if the given task is solvable with the available tools.

      Task: {{.query}}

      Please analyze:
      1. Is the task clearly defined and understandable?
      2. Are the required tools available to complete this task?
      3. Is the task within the scope of what can be accomplished with the given toolset?
      4. Are there any fundamental limitations that would prevent task completion?

      Respond with one of the following:
      - "Solvable": The task can be completed with the available tools
      - "Unsolvable": The task cannot be completed due to missing tools or fundamental limitations
      - "Unsure": It is unclear whether the task can be completed
    functions:
      - "ai:list_tools"
      - "ai:get_tool_info"

  - name: "select_answer"
    display: "Select Better Answer"
    description: |
      This valuation assistant evaluates and selects the better of two answers to a query.
      It assesses completeness, accuracy, tool usage, reasoning, and final result.
      The response includes a chosen index and justification.
    model: "llm/any"
    instruction: |
      #! mime-type=text/x-go-template
      You are an evaluation assistant.
      Please compare multiple answers to the same query and select the better one.

      Query: {{.query}}

      Answers: {{.result}}

      Please compare these answers based on:
      1. Completeness: Which answer more thoroughly addresses the query?
      2. Accuracy: Which answer provides more correct information?
      3. Tool Usage: Which answer demonstrates better tool selection and usage?
      4. Reasoning: Which answer shows clearer logical thinking?
      5. Final Result: Which answer provides a better final solution?

      Respond with the index of the better answer (0 or 1) and a brief explanation of your choice.

      Format your response as:
      Index: [0 or 1]
      Reason: [brief explanation]

###
set: "llm"
provider: "openai"
base_url: "https://api.openai.com/v1/"
api_key: "openai"
models:
  any:
    model: "gpt-5-mini"
###
